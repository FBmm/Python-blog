# 爬虫

百度百科
> 网络爬虫是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。

网络爬虫（又称网络蜘蛛，机器人），就是模拟客户端发送网络请求，接收请求响应，一种按照一定的规则，自动地抓取互联网信息的程序。
只要浏览器能够做的事情，原则上，爬虫都能够做到。

简单来说，爬虫就是自动从网络上收集信息的一种程序，复杂点来说，就是一整套关于数据请求、处理、存储的程序。

## 原理

发送请求 -> 接收响应 -> 解析网页 (-> 提取链接 -> 发送请求) -> 提取资源 -> 保存资源

### 发送请求

实质是模拟浏览器发送 http 请求报文的过程，实际中只需要请求服务器需要的部分报文。

#### 请求库

模拟浏览器的请求

1. requests 爬虫最常用的库
    比起之前用到的urllib，requests模块的api更加便捷（本质就是封装了urllib3）。

2. urllib
    提供了一系列用于操作URL的功能。

#### 请求报文

请求行、请求头、空行、请求体
![Alt](./assets/request-message.jpg#pic_center)

[使用Fiddler抓取http请求(抓包)](https://www.cnblogs.com/yyhh/p/5140852.html)

一个完整的 http 请求报文

```http
POST http://columbus.os.adc.com/api/coop/requirement/tree/query/prune/page?projectId=100001 HTTP/1.1
Host: columbus.os.adc.com
Connection: keep-alive
Content-Length: 483
Accept: application/json, text/plain, */*
Origin: http://columbus.os.adc.com
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3314.0 Safari/537.36 SE 2.X MetaSr 1.0
Content-Type: application/json;charset=UTF-8;multipart/form-data
Referer: http://columbus.os.adc.com/requirement/list?projectId=100001
Accept-Encoding: gzip, deflate
Accept-Language: zh-CN,zh;q=0.9
Cookie: SESSION=1e365567-bbed-4ff8-b033-6ab21dc05bd2; user=W9005349

{"projectId":"100001","title":"","assignUsers":[],"isArchived":-1}
```

##### 请求行

请求方法 + 空格 +请求URL + 空格 + HTTP协议版本 + 回车 换行

##### 请求头

请求的配置信息，key-value 形式发送给服务器

> Accept： 浏览器可接受的MIME类型。
> Accept-Charset：浏览器可接受的字符集。
> Accept-Encoding：浏览器能够进行解码的数据编码方式，比如gzip。Servlet能够向支持gzip    的浏览器返回经gzip编码的HTML页面。许多情形下这可以减少5到10倍的下载时间。
> Accept-Language：浏览器所希望的语言种类，当服务器能够提供一种以上的语言版本时要用到。
> Content-Length：表示请求消息正文的长度。
> Host： 客户机通过这个头告诉服务器，想访问的主机名。Host头域指定请求资源的Intenet主机和端口号，必须表示请求url的原始服务器或网关的位置。HTTP/1.1请求必须包含主机头域，否则系统会以400状态码返回。
> If-Modified-Since：客户机通过这个头告诉服务器，资源的缓存时间。只有当所请求的内容在指定的时间后又经过修改才返回它，否则返回304“Not Modified”应答。
> Referer：客户机通过这个头告诉服务器，它是从哪个资源来访问服务器的(防盗链)。包含一个URL，用户从该URL代表的页面出发访问当前请求的页面。
> User-Agent：User-Agent头域的内容包含发出请求的用户信息。浏览器类型，如果Servlet返回的内容与浏览器类型有关则该值非常有用。
> Cookie：客户机通过这个头可以向服务器带数据，这是最重要的请求头信息之一。
> Cookie2：用来说明请求端支持的cookie版本。
> Range：如果服务器支持范围请求，就请求资源的指定范围。

##### 空行

通过空行告诉服务器请求头部结束。

##### 请求体

根据不同的请求方法包含不同的内容。
get: 空
post: 表单数据

### 接收响应

获取特定请求返回的响应报文，提取目标数据。

#### 响应报文

响应行、响应头、空行、响应体
![Alt](./assets/response-message.jpg#pic_center)

一个完整的 http 响应报文

```http
HTTP/1.1 200
Server: nginx/1.12.2
Date: Thu, 23 Jan 2020 02:03:03 GMT
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Connection: keep-alive
trace-id: 15-32349ee9-8945-438c-91a6-33ff10c0c0ee-18882
X-Content-Encoding-Over-Network: gzip

77
{"status":200,"msg":null,"data":{"result":[],"pageInfo":{"totalPages":0,"totalRecords":0,"pageNumber":1,"pageSize":5}}}
0
```

##### 响应行

http协议版本 + 空格 + 状态码 + 空格 + 状态码描述 + 回车 换行

http状态码

1. 100~199：信息，服务器收到请求，需要请求者继续执行操作

2. 200~299：成功，操作被成功接收并处理

3. 300~399：重定向，需要进一步的操作以完成请求

4. 400~499：客户端错误，请求包含语法错误或无法完成请求

5. 500~599：服务器错误，服务器在处理请求的过程中发生错误

##### 响应头

描述服务器和数据的基本信息。

Set-Cookie：设置浏览器 Cookie，以后当浏览器访问符合条件的 URL 时，会自动带上该 Cooike
Refresh：告诉浏览器隔多久刷新一次，以秒计

##### 空行

响应头结束

##### 响应体

网站返回的数据

### 解析网页

1. 提取网页链接

2. 提取网页资源

#### 解析库

源码中找到并提取数据

1. Beautiful Soup
    Beautiful Soup 是 python 的一个库，其最主要的功能是从网页中抓取数据。

2. pyquery
    据说比beautiful 好用，语法和jquery非常像。

## 爬虫开源框架

1. scrapy：是Python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。

2. nutch： 是一个开源Java实现的搜索引擎。它提供了我们运行自己的搜索引擎所需的全部工具。包括全文搜索和Web爬虫。

## 商业场景

1. 搜索引擎
    百度、谷歌搜索就是一个巨大的爬虫程序。

2. 爬取新闻 新闻门户网站的新闻共享
    比如说今日头条、一点新闻、腾讯新闻等等。他们的新闻来源，一个当然是从自己的记者这里产生，另一部分呢，就是从其他网站上，爬下来他们的记者写的文章。这样才能够做到新闻的门户级别。一个网站报道了一个新闻，其他的网站马上会给你推送相同的新闻了。这中间就是爬虫在起作用，如果是人去找新闻，不仅速度慢，还可能会重复，而且不能24小时工作。这当然就很不方便了。

3. 为机器学习，大数据做准备
    机器学习基于海量的数据样本，比如人脸识别，图像识别，语音识别，只有对大量的样本数据进行分析，才能提升程序的识别准确度。至于大数据分析更是基于海量数据，所以爬虫就是获取数据的一种途径。

## 个人用途

1. 分析喜欢妹子的朋友圈hh

2. 找工作时爬取招聘网站某工种的所有招聘信息
